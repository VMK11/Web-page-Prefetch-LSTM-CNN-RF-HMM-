{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('pool-out.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IP</th>\n",
       "      <th>pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0.180.125</td>\n",
       "      <td>/components</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.1.49.175</td>\n",
       "      <td>/effective,/community,/treatment,/professional...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.10.61.147</td>\n",
       "      <td>/especially,/application,/especially,/associat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.10.9.113</td>\n",
       "      <td>/enterprise,/something</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.103.207.46</td>\n",
       "      <td>/authority,/discussion,/associates,/membership...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             IP                                              pages\n",
       "0   1.0.180.125                                        /components\n",
       "1    1.1.49.175  /effective,/community,/treatment,/professional...\n",
       "2   1.10.61.147  /especially,/application,/especially,/associat...\n",
       "3    1.10.9.113                             /enterprise,/something\n",
       "4  1.103.207.46  /authority,/discussion,/associates,/membership..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df.pages.apply(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = max([len(x) for x in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = ','.join(df.pages)\n",
    "le = LabelEncoder()\n",
    "le.fit(all_data.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(data):\n",
    "    data[i] = le.transform(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for item in data:\n",
    "    if len(item) > 1:\n",
    "        X.append(item[:-1])\n",
    "        y.append(item[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_d = []\n",
    "for i, item in enumerate(X):\n",
    "    l_d.append(len(X[i]))\n",
    "    tmp = list(X[i])\n",
    "    while len(tmp) < MAX_LEN:\n",
    "        tmp.append(np.int64(0))\n",
    "    tmp = tmp[:MAX_LEN]\n",
    "    X[i] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "l_d = np.array(l_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "ind = np.arange(len(X))\n",
    "np.random.shuffle(ind)\n",
    "\n",
    "X = X[ind]\n",
    "y = y[ind]\n",
    "l_d = l_d[ind]\n",
    "\n",
    "train_data = X[:int(len(X)*0.8)]\n",
    "train_y = y[:int(len(y)*0.8)]\n",
    "train_l_d = l_d[:int(len(l_d)*0.8)]\n",
    "\n",
    "test_data = X[int(len(X)*0.8):]\n",
    "test_y = y[int(len(y)*0.8):]\n",
    "test_l_d = l_d[int(len(l_d)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_y.reshape(-1, 1)\n",
    "test_y = test_y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh = OneHotEncoder(n_values='auto', sparse=False)\n",
    "oh.fit(list(train_y))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = oh.transform(train_y)\n",
    "test_y = oh.transform(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X, y, l_d, batch_size, rnd=True):\n",
    "    b_X = [] \n",
    "    b_y = []\n",
    "    b_X_sizes = [] \n",
    "    \n",
    "    if rnd:\n",
    "        ind = np.arange(len(X))\n",
    "        np.random.shuffle(ind)\n",
    "        X = X[ind]\n",
    "        y = y[ind]\n",
    "        l_d = l_d[ind]\n",
    "    \n",
    "    i = 0\n",
    "    for x, l, s in zip(X, y, l_d): \n",
    "        if len(b_X) == batch_size:\n",
    "            b_X = [] \n",
    "            b_y = []\n",
    "            b_X_sizes = [] \n",
    "            \n",
    "        b_X.append(x)\n",
    "        b_y.append(l)\n",
    "        b_X_sizes.append(s)\n",
    "        i += 1\n",
    "        \n",
    "        if i+1 > len(X):\n",
    "            yield np.array(b_X), np.array(b_y), np.array(b_X_sizes)\n",
    "        \n",
    "        if len(b_X) < batch_size:\n",
    "            continue\n",
    "                \n",
    "        yield np.array(b_X), np.array(b_y), np.array(b_X_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 50\n",
    "embedding_matrix = np.random.uniform(-np.sqrt(3/dim), np.sqrt(3/dim), (len(le.classes_)+1, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-28-056555940706>:27: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:110: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "pages = tf.placeholder(tf.int32, shape=[None, MAX_LEN]) # Input data\n",
    "pages_lengths = tf.placeholder(tf.int32, shape=[None]) # Real langths for inpur data - for padding\n",
    "\n",
    "dropout = tf.placeholder(dtype=tf.float32, shape=[]) # Placeholder to set dropout amount\n",
    "\n",
    "W = tf.Variable(embedding_matrix, dtype=tf.float32)\n",
    "embeddings = tf.nn.embedding_lookup(W, pages) # Use embeddings for pages - it allow to use additional info\n",
    "\n",
    "embeddings = tf.nn.dropout(embeddings, dropout) # Add dropout\n",
    "\n",
    "# Create LSTM layer with masking padds and set it to return only last out\n",
    "lstm_cell = tf.nn.rnn_cell.LSTMCell(200, state_is_tuple=True)\n",
    "_, (_, out) = tf.nn.dynamic_rnn(lstm_cell, embeddings, sequence_length=pages_lengths, dtype=tf.float32)\n",
    "\n",
    "out = tf.nn.dropout(out, dropout) # Add dropout\n",
    "\n",
    "# Addinionaly layer for deeper representation\n",
    "out = tf.layers.dense(out, 100, activation=tf.nn.relu, kernel_initializer=tf.keras.initializers.he_normal())\n",
    "out = tf.nn.dropout(out, dropout) # Add dropout\n",
    "\n",
    "# Output layer with number of units equal to number of classes\n",
    "# Use no activation because of softmax_cross_entropy_with_logits\n",
    "out = tf.layers.dense(out, len(le.classes_)) \n",
    "ans = tf.nn.softmax(out) # Add softmax activation for used output of NN\n",
    "\n",
    "labels = tf.placeholder(tf.int32, shape=[None, len(le.classes_)]) # Real next pages for train\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=out))\n",
    "\n",
    "# Compute accuracy\n",
    "correct_predictions = tf.equal(tf.argmax(labels, 1), tf.argmax(ans, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, 'float'))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.001)\n",
    "gvs = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_norm(grad, 1.0), var) for grad, var in gvs] # Because our NN is Recurrent\n",
    "train_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20864/20858 Epoch: 1, loss: 5.185360, test_loss: 5.111279 acc: 0.010746, test_acc: 0.011122\n",
      "20864/20858 Epoch: 2, loss: 5.105743, test_loss: 5.101874 acc: 0.009878, test_acc: 0.012272\n",
      "20864/20858 Epoch: 3, loss: 5.095975, test_loss: 5.100788 acc: 0.011266, test_acc: 0.011505\n",
      "20864/20858 Epoch: 4, loss: 5.091166, test_loss: 5.099844 acc: 0.011798, test_acc: 0.010930\n",
      "20864/20858 Epoch: 5, loss: 5.083949, test_loss: 5.098670 acc: 0.011611, test_acc: 0.009779\n",
      "20864/20858 Epoch: 6, loss: 5.081219, test_loss: 5.099067 acc: 0.012272, test_acc: 0.010355\n",
      "20864/20858 Epoch: 7, loss: 5.077614, test_loss: 5.100949 acc: 0.013518, test_acc: 0.010930\n",
      "20864/20858 Epoch: 8, loss: 5.075349, test_loss: 5.102275 acc: 0.012229, test_acc: 0.009779\n",
      "20864/20858 Epoch: 9, loss: 5.069973, test_loss: 5.104515 acc: 0.012510, test_acc: 0.008821\n",
      "20864/20858 Epoch: 10, loss: 5.062792, test_loss: 5.105771 acc: 0.014338, test_acc: 0.011314\n",
      "20864/20858 Epoch: 11, loss: 5.059787, test_loss: 5.107564 acc: 0.013382, test_acc: 0.011122\n",
      "20864/20858 Epoch: 12, loss: 5.056047, test_loss: 5.108151 acc: 0.014860, test_acc: 0.009779\n",
      "20864/20858 Epoch: 13, loss: 5.052053, test_loss: 5.111139 acc: 0.016250, test_acc: 0.009779\n",
      "20864/20858 Epoch: 14, loss: 5.046367, test_loss: 5.113603 acc: 0.016447, test_acc: 0.011122\n",
      "20864/20858 Epoch: 15, loss: 5.044428, test_loss: 5.113434 acc: 0.016536, test_acc: 0.012656\n",
      "20864/20858 Epoch: 16, loss: 5.036897, test_loss: 5.116592 acc: 0.016447, test_acc: 0.010547\n",
      "20864/20858 Epoch: 17, loss: 5.033401, test_loss: 5.118159 acc: 0.015824, test_acc: 0.009588\n",
      "20864/20858 Epoch: 18, loss: 5.028359, test_loss: 5.119044 acc: 0.016540, test_acc: 0.011505\n",
      "20864/20858 Epoch: 19, loss: 5.025130, test_loss: 5.119986 acc: 0.016018, test_acc: 0.010355\n",
      "20864/20858 Epoch: 20, loss: 5.018373, test_loss: 5.123826 acc: 0.016969, test_acc: 0.009204\n",
      "20864/20858 Epoch: 21, loss: 5.015486, test_loss: 5.124170 acc: 0.017592, test_acc: 0.009204\n",
      "20864/20858 Epoch: 22, loss: 5.014153, test_loss: 5.125844 acc: 0.018652, test_acc: 0.009396\n",
      "20864/20858 Epoch: 23, loss: 5.005163, test_loss: 5.129041 acc: 0.019179, test_acc: 0.008629\n",
      "20864/20858 Epoch: 24, loss: 5.001846, test_loss: 5.132156 acc: 0.021096, test_acc: 0.009396\n",
      "20864/20858 Epoch: 25, loss: 5.000474, test_loss: 5.133494 acc: 0.018458, test_acc: 0.008821\n",
      "20864/20858 Epoch: 26, loss: 4.995350, test_loss: 5.134030 acc: 0.020372, test_acc: 0.009012\n",
      "20864/20858 Epoch: 27, loss: 4.989233, test_loss: 5.139642 acc: 0.020473, test_acc: 0.010547\n",
      "20864/20858 Epoch: 28, loss: 4.989161, test_loss: 5.139404 acc: 0.020425, test_acc: 0.008054\n",
      "20864/20858 Epoch: 29, loss: 4.981214, test_loss: 5.141920 acc: 0.019172, test_acc: 0.009588\n",
      "20864/20858 Epoch: 30, loss: 4.978726, test_loss: 5.142278 acc: 0.022484, test_acc: 0.009971\n",
      "20864/20858 Epoch: 31, loss: 4.972830, test_loss: 5.144352 acc: 0.020902, test_acc: 0.009396\n",
      "20864/20858 Epoch: 32, loss: 4.970624, test_loss: 5.152662 acc: 0.022055, test_acc: 0.011314\n",
      "20864/20858 Epoch: 33, loss: 4.966437, test_loss: 5.154235 acc: 0.020523, test_acc: 0.009204\n",
      "20864/20858 Epoch: 34, loss: 4.962720, test_loss: 5.154960 acc: 0.021530, test_acc: 0.010547\n",
      "20864/20858 Epoch: 35, loss: 4.955728, test_loss: 5.157152 acc: 0.021578, test_acc: 0.010930\n",
      "20864/20858 Epoch: 36, loss: 4.955788, test_loss: 5.161553 acc: 0.022970, test_acc: 0.011697\n",
      "20864/20858 Epoch: 37, loss: 4.952672, test_loss: 5.163808 acc: 0.023888, test_acc: 0.010738\n",
      "20864/20858 Epoch: 38, loss: 4.951487, test_loss: 5.162230 acc: 0.022582, test_acc: 0.010738\n",
      "20864/20858 Epoch: 39, loss: 4.942104, test_loss: 5.166173 acc: 0.024164, test_acc: 0.009204\n",
      "20864/20858 Epoch: 40, loss: 4.942121, test_loss: 5.169205 acc: 0.022771, test_acc: 0.011122\n",
      "20864/20858 Epoch: 41, loss: 4.940937, test_loss: 5.168451 acc: 0.023536, test_acc: 0.011314\n",
      "20864/20858 Epoch: 42, loss: 4.934442, test_loss: 5.174766 acc: 0.024367, test_acc: 0.010738\n",
      "20864/20858 Epoch: 43, loss: 4.926694, test_loss: 5.175972 acc: 0.024259, test_acc: 0.010738\n",
      "20864/20858 Epoch: 44, loss: 4.930234, test_loss: 5.172714 acc: 0.024211, test_acc: 0.011505\n",
      "20864/20858 Epoch: 45, loss: 4.920137, test_loss: 5.183850 acc: 0.025791, test_acc: 0.011122\n",
      "20864/20858 Epoch: 46, loss: 4.916221, test_loss: 5.173903 acc: 0.025553, test_acc: 0.011122\n",
      "20864/20858 Epoch: 47, loss: 4.909987, test_loss: 5.184750 acc: 0.026507, test_acc: 0.010930\n",
      "20864/20858 Epoch: 48, loss: 4.912658, test_loss: 5.181728 acc: 0.027804, test_acc: 0.012081\n",
      "20864/20858 Epoch: 49, loss: 4.908036, test_loss: 5.187855 acc: 0.026270, test_acc: 0.011314\n",
      "20864/20858 Epoch: 50, loss: 4.904850, test_loss: 5.184484 acc: 0.027478, test_acc: 0.010163\n",
      "20864/20858 Epoch: 51, loss: 4.896313, test_loss: 5.188892 acc: 0.029580, test_acc: 0.010355\n",
      "20864/20858 Epoch: 52, loss: 4.897321, test_loss: 5.187926 acc: 0.027035, test_acc: 0.010163\n",
      "20864/20858 Epoch: 53, loss: 4.895247, test_loss: 5.192164 acc: 0.025122, test_acc: 0.009971\n",
      "20864/20858 Epoch: 54, loss: 4.890483, test_loss: 5.191481 acc: 0.028000, test_acc: 0.010547\n",
      "20864/20858 Epoch: 55, loss: 4.887541, test_loss: 5.188504 acc: 0.027087, test_acc: 0.009588\n",
      "20864/20858 Epoch: 56, loss: 4.879033, test_loss: 5.195161 acc: 0.027233, test_acc: 0.011122\n",
      "20864/20858 Epoch: 57, loss: 4.879150, test_loss: 5.192242 acc: 0.028956, test_acc: 0.009588\n",
      "20864/20858 Epoch: 58, loss: 4.873325, test_loss: 5.205704 acc: 0.029532, test_acc: 0.011122\n",
      "20864/20858 Epoch: 59, loss: 4.875944, test_loss: 5.208421 acc: 0.029055, test_acc: 0.009971\n",
      "20864/20858 Epoch: 60, loss: 4.867068, test_loss: 5.207812 acc: 0.027852, test_acc: 0.010355\n",
      "20864/20858 Epoch: 61, loss: 4.865193, test_loss: 5.205376 acc: 0.029675, test_acc: 0.011314\n",
      "20864/20858 Epoch: 62, loss: 4.865267, test_loss: 5.216724 acc: 0.029390, test_acc: 0.013615\n",
      "20864/20858 Epoch: 63, loss: 4.863845, test_loss: 5.210404 acc: 0.030198, test_acc: 0.010930\n",
      "20864/20858 Epoch: 64, loss: 4.848332, test_loss: 5.224378 acc: 0.032125, test_acc: 0.010738\n",
      "20864/20858 Epoch: 65, loss: 4.863651, test_loss: 5.206222 acc: 0.030682, test_acc: 0.010547\n",
      "20864/20858 Epoch: 66, loss: 4.847012, test_loss: 5.229381 acc: 0.029869, test_acc: 0.011314\n",
      "20864/20858 Epoch: 67, loss: 4.848885, test_loss: 5.222849 acc: 0.031458, test_acc: 0.010547\n",
      "20864/20858 Epoch: 68, loss: 4.845349, test_loss: 5.223194 acc: 0.031020, test_acc: 0.011505\n",
      "20864/20858 Epoch: 69, loss: 4.841555, test_loss: 5.231472 acc: 0.030871, test_acc: 0.011505\n",
      "20864/20858 Epoch: 70, loss: 4.843824, test_loss: 5.227978 acc: 0.031250, test_acc: 0.009779\n",
      "20864/20858 Epoch: 71, loss: 4.837118, test_loss: 5.229451 acc: 0.029007, test_acc: 0.010547\n",
      "20864/20858 Epoch: 72, loss: 4.833978, test_loss: 5.227054 acc: 0.032029, test_acc: 0.010355\n",
      "20864/20858 Epoch: 73, loss: 4.828949, test_loss: 5.246313 acc: 0.031930, test_acc: 0.009396\n",
      "20864/20858 Epoch: 74, loss: 4.821703, test_loss: 5.230392 acc: 0.032506, test_acc: 0.009396\n",
      "20864/20858 Epoch: 75, loss: 4.829298, test_loss: 5.238788 acc: 0.032410, test_acc: 0.011122\n",
      "20864/20858 Epoch: 76, loss: 4.825964, test_loss: 5.244153 acc: 0.034372, test_acc: 0.009588\n",
      "20864/20858 Epoch: 77, loss: 4.820046, test_loss: 5.238157 acc: 0.031971, test_acc: 0.011697\n",
      "20864/20858 Epoch: 78, loss: 4.811874, test_loss: 5.241658 acc: 0.035146, test_acc: 0.010163\n",
      "20864/20858 Epoch: 79, loss: 4.817672, test_loss: 5.241190 acc: 0.033222, test_acc: 0.010355\n",
      "20864/20858 Epoch: 80, loss: 4.807157, test_loss: 5.250051 acc: 0.035084, test_acc: 0.010930\n",
      "20864/20858 Epoch: 81, loss: 4.803596, test_loss: 5.255696 acc: 0.033373, test_acc: 0.010355\n",
      "20864/20858 Epoch: 82, loss: 4.797873, test_loss: 5.244744 acc: 0.036532, test_acc: 0.012081\n",
      "20864/20858 Epoch: 83, loss: 4.802491, test_loss: 5.258240 acc: 0.033948, test_acc: 0.011505\n",
      "20864/20858 Epoch: 84, loss: 4.800684, test_loss: 5.257175 acc: 0.035621, test_acc: 0.009971\n",
      "20864/20858 Epoch: 85, loss: 4.793842, test_loss: 5.262270 acc: 0.036865, test_acc: 0.009779\n",
      "20864/20858 Epoch: 86, loss: 4.799652, test_loss: 5.261749 acc: 0.036246, test_acc: 0.009588\n",
      "20864/20858 Epoch: 87, loss: 4.790056, test_loss: 5.251399 acc: 0.037294, test_acc: 0.010163\n",
      "20864/20858 Epoch: 88, loss: 4.796724, test_loss: 5.257313 acc: 0.035621, test_acc: 0.011122\n",
      "20864/20858 Epoch: 89, loss: 4.790555, test_loss: 5.257461 acc: 0.037308, test_acc: 0.009779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20864/20858 Epoch: 90, loss: 4.779900, test_loss: 5.267778 acc: 0.034996, test_acc: 0.010930\n",
      "20864/20858 Epoch: 91, loss: 4.786793, test_loss: 5.263425 acc: 0.037061, test_acc: 0.009588\n",
      "20864/20858 Epoch: 92, loss: 4.776484, test_loss: 5.270391 acc: 0.037550, test_acc: 0.009971\n",
      "20864/20858 Epoch: 93, loss: 4.770185, test_loss: 5.268651 acc: 0.037732, test_acc: 0.009012\n",
      "20864/20858 Epoch: 94, loss: 4.777714, test_loss: 5.270579 acc: 0.034413, test_acc: 0.011314\n",
      "20864/20858 Epoch: 95, loss: 4.768670, test_loss: 5.268251 acc: 0.038842, test_acc: 0.009204\n",
      "20864/20858 Epoch: 96, loss: 4.780897, test_loss: 5.266306 acc: 0.038542, test_acc: 0.010163\n",
      "20864/20858 Epoch: 97, loss: 4.774036, test_loss: 5.269674 acc: 0.037493, test_acc: 0.009204\n",
      "20864/20858 Epoch: 98, loss: 4.755531, test_loss: 5.278616 acc: 0.038116, test_acc: 0.009779\n",
      "20864/20858 Epoch: 99, loss: 4.760998, test_loss: 5.274628 acc: 0.040284, test_acc: 0.009779\n",
      "20864/20858 Epoch: 100, loss: 4.759270, test_loss: 5.279488 acc: 0.038269, test_acc: 0.009588\n",
      "20864/20858 Epoch: 101, loss: 4.759478, test_loss: 5.285721 acc: 0.038981, test_acc: 0.009204\n",
      "20864/20858 Epoch: 102, loss: 4.753639, test_loss: 5.291754 acc: 0.038362, test_acc: 0.008245\n",
      "20864/20858 Epoch: 103, loss: 4.752461, test_loss: 5.295078 acc: 0.040229, test_acc: 0.008629\n",
      "20864/20858 Epoch: 104, loss: 4.747828, test_loss: 5.288198 acc: 0.040515, test_acc: 0.009588\n",
      "20864/20858 Epoch: 105, loss: 4.741867, test_loss: 5.293131 acc: 0.040222, test_acc: 0.009396\n",
      "20864/20858 Epoch: 106, loss: 4.746669, test_loss: 5.298772 acc: 0.040656, test_acc: 0.009971\n",
      "20864/20858 Epoch: 107, loss: 4.731743, test_loss: 5.307453 acc: 0.042858, test_acc: 0.007862\n",
      "20864/20858 Epoch: 108, loss: 4.738111, test_loss: 5.299464 acc: 0.040649, test_acc: 0.009012\n",
      "20864/20858 Epoch: 109, loss: 4.734829, test_loss: 5.309182 acc: 0.039937, test_acc: 0.008629\n",
      "20864/20858 Epoch: 110, loss: 4.724077, test_loss: 5.310563 acc: 0.040227, test_acc: 0.008629\n",
      "20864/20858 Epoch: 111, loss: 4.730449, test_loss: 5.309733 acc: 0.040948, test_acc: 0.009396\n",
      "20864/20858 Epoch: 112, loss: 4.717981, test_loss: 5.320259 acc: 0.040407, test_acc: 0.008629\n",
      "20864/20858 Epoch: 113, loss: 4.724490, test_loss: 5.307230 acc: 0.042520, test_acc: 0.008054\n",
      "20864/20858 Epoch: 114, loss: 4.716008, test_loss: 5.324112 acc: 0.044497, test_acc: 0.009588\n",
      "20864/20858 Epoch: 115, loss: 4.720126, test_loss: 5.321378 acc: 0.041432, test_acc: 0.009971\n",
      "20864/20858 Epoch: 116, loss: 4.721493, test_loss: 5.314160 acc: 0.039462, test_acc: 0.009396\n",
      "20864/20858 Epoch: 117, loss: 4.717818, test_loss: 5.330196 acc: 0.041710, test_acc: 0.008629\n",
      "20864/20858 Epoch: 118, loss: 4.711655, test_loss: 5.318389 acc: 0.045492, test_acc: 0.008821\n",
      "20864/20858 Epoch: 119, loss: 4.708091, test_loss: 5.314880 acc: 0.044155, test_acc: 0.008245\n",
      "20864/20858 Epoch: 120, loss: 4.709379, test_loss: 5.322172 acc: 0.047172, test_acc: 0.010163\n",
      "20864/20858 Epoch: 121, loss: 4.701000, test_loss: 5.307879 acc: 0.044308, test_acc: 0.009971\n",
      "20864/20858 Epoch: 122, loss: 4.704350, test_loss: 5.315606 acc: 0.044447, test_acc: 0.009396\n",
      "20864/20858 Epoch: 123, loss: 4.695856, test_loss: 5.327274 acc: 0.046697, test_acc: 0.010738\n",
      "20864/20858 Epoch: 124, loss: 4.702106, test_loss: 5.327621 acc: 0.043584, test_acc: 0.010163\n",
      "20864/20858 Epoch: 125, loss: 4.700899, test_loss: 5.341006 acc: 0.045739, test_acc: 0.009204\n",
      "20864/20858 Epoch: 126, loss: 4.699417, test_loss: 5.340390 acc: 0.047225, test_acc: 0.009779\n",
      "20864/20858 Epoch: 127, loss: 4.697943, test_loss: 5.332033 acc: 0.047366, test_acc: 0.010547\n",
      "20864/20858 Epoch: 128, loss: 4.692150, test_loss: 5.336881 acc: 0.046362, test_acc: 0.010547\n",
      "20864/20858 Epoch: 129, loss: 4.692487, test_loss: 5.340351 acc: 0.044965, test_acc: 0.010930\n",
      "20864/20858 Epoch: 130, loss: 4.685133, test_loss: 5.325612 acc: 0.048339, test_acc: 0.012081\n",
      "20864/20858 Epoch: 131, loss: 4.688511, test_loss: 5.330634 acc: 0.047129, test_acc: 0.009971\n",
      "20864/20858 Epoch: 132, loss: 4.689285, test_loss: 5.343908 acc: 0.045933, test_acc: 0.011505\n",
      "20864/20858 Epoch: 133, loss: 4.682576, test_loss: 5.348534 acc: 0.047457, test_acc: 0.009779\n",
      "20864/20858 Epoch: 134, loss: 4.685040, test_loss: 5.347672 acc: 0.044097, test_acc: 0.009204\n",
      "20864/20858 Epoch: 135, loss: 4.673344, test_loss: 5.358267 acc: 0.045061, test_acc: 0.009396\n",
      "20864/20858 Epoch: 136, loss: 4.671354, test_loss: 5.354172 acc: 0.045780, test_acc: 0.009588\n",
      "20864/20858 Epoch: 137, loss: 4.670969, test_loss: 5.346441 acc: 0.047752, test_acc: 0.010930\n",
      "20864/20858 Epoch: 138, loss: 4.665143, test_loss: 5.351321 acc: 0.048473, test_acc: 0.009779\n",
      "20864/20858 Epoch: 139, loss: 4.664449, test_loss: 5.360528 acc: 0.051251, test_acc: 0.010547\n",
      "20864/20858 Epoch: 140, loss: 4.673095, test_loss: 5.345718 acc: 0.046980, test_acc: 0.010163\n",
      "20864/20858 Epoch: 141, loss: 4.664331, test_loss: 5.356203 acc: 0.048226, test_acc: 0.010163\n",
      "20864/20858 Epoch: 142, loss: 4.656396, test_loss: 5.365303 acc: 0.050819, test_acc: 0.009779\n",
      "20864/20858 Epoch: 143, loss: 4.666770, test_loss: 5.358441 acc: 0.046700, test_acc: 0.008821\n",
      "20864/20858 Epoch: 144, loss: 4.652056, test_loss: 5.375882 acc: 0.051728, test_acc: 0.008245\n",
      "20864/20858 Epoch: 145, loss: 4.660890, test_loss: 5.361346 acc: 0.048277, test_acc: 0.008821\n",
      "20864/20858 Epoch: 146, loss: 4.653208, test_loss: 5.368877 acc: 0.048857, test_acc: 0.009396\n",
      "20864/20858 Epoch: 147, loss: 4.651560, test_loss: 5.364806 acc: 0.049614, test_acc: 0.008054\n",
      "20864/20858 Epoch: 148, loss: 4.650127, test_loss: 5.362439 acc: 0.048416, test_acc: 0.010930\n",
      "20864/20858 Epoch: 149, loss: 4.648913, test_loss: 5.363595 acc: 0.049190, test_acc: 0.009204\n",
      "20864/20858 Epoch: 150, loss: 4.650032, test_loss: 5.362164 acc: 0.048226, test_acc: 0.009204\n",
      "20864/20858 Epoch: 151, loss: 4.647395, test_loss: 5.370938 acc: 0.050474, test_acc: 0.009396\n",
      "20864/20858 Epoch: 152, loss: 4.638113, test_loss: 5.382195 acc: 0.052401, test_acc: 0.008821\n",
      "20864/20858 Epoch: 153, loss: 4.640335, test_loss: 5.388592 acc: 0.047122, test_acc: 0.009588\n",
      "20864/20858 Epoch: 154, loss: 4.645880, test_loss: 5.359789 acc: 0.048377, test_acc: 0.009971\n",
      "20864/20858 Epoch: 155, loss: 4.639761, test_loss: 5.380652 acc: 0.050726, test_acc: 0.009396\n",
      "20864/20858 Epoch: 156, loss: 4.627236, test_loss: 5.390550 acc: 0.053170, test_acc: 0.009396\n",
      "20864/20858 Epoch: 157, loss: 4.635200, test_loss: 5.372533 acc: 0.050580, test_acc: 0.008821\n",
      "20864/20858 Epoch: 158, loss: 4.635249, test_loss: 5.381939 acc: 0.051021, test_acc: 0.010163\n",
      "20864/20858 Epoch: 159, loss: 4.633283, test_loss: 5.394753 acc: 0.051776, test_acc: 0.009396\n",
      "20864/20858 Epoch: 160, loss: 4.634067, test_loss: 5.379324 acc: 0.051490, test_acc: 0.010547\n",
      "20864/20858 Epoch: 161, loss: 4.625934, test_loss: 5.389041 acc: 0.052262, test_acc: 0.009779\n",
      "20864/20858 Epoch: 162, loss: 4.625644, test_loss: 5.399617 acc: 0.051680, test_acc: 0.011122\n",
      "20864/20858 Epoch: 163, loss: 4.615666, test_loss: 5.408955 acc: 0.051582, test_acc: 0.009396\n",
      "20864/20858 Epoch: 164, loss: 4.623933, test_loss: 5.392467 acc: 0.052588, test_acc: 0.010930\n",
      "20864/20858 Epoch: 165, loss: 4.624274, test_loss: 5.401871 acc: 0.052027, test_acc: 0.009396\n",
      "20864/20858 Epoch: 166, loss: 4.615160, test_loss: 5.393287 acc: 0.051922, test_acc: 0.009204\n",
      "20864/20858 Epoch: 167, loss: 4.621429, test_loss: 5.399921 acc: 0.054553, test_acc: 0.010355\n",
      "20864/20858 Epoch: 168, loss: 4.611713, test_loss: 5.406191 acc: 0.053599, test_acc: 0.010355\n",
      "20864/20858 Epoch: 169, loss: 4.605100, test_loss: 5.424446 acc: 0.054076, test_acc: 0.007862\n",
      "20864/20858 Epoch: 170, loss: 4.610161, test_loss: 5.411017 acc: 0.053650, test_acc: 0.008437\n",
      "20864/20858 Epoch: 171, loss: 4.610097, test_loss: 5.420297 acc: 0.055900, test_acc: 0.008054\n",
      "20864/20858 Epoch: 172, loss: 4.607121, test_loss: 5.417907 acc: 0.050822, test_acc: 0.008054\n",
      "20864/20858 Epoch: 173, loss: 4.606159, test_loss: 5.410190 acc: 0.054560, test_acc: 0.009204\n",
      "20864/20858 Epoch: 174, loss: 4.602772, test_loss: 5.415304 acc: 0.055483, test_acc: 0.009588\n",
      "20864/20858 Epoch: 175, loss: 4.599818, test_loss: 5.420599 acc: 0.052257, test_acc: 0.009396\n",
      "20864/20858 Epoch: 176, loss: 4.612364, test_loss: 5.399655 acc: 0.051205, test_acc: 0.009779\n",
      "20864/20858 Epoch: 177, loss: 4.593421, test_loss: 5.417758 acc: 0.056863, test_acc: 0.009779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20864/20858 Epoch: 178, loss: 4.599121, test_loss: 5.416360 acc: 0.056389, test_acc: 0.008821\n",
      "20864/20858 Epoch: 179, loss: 4.584983, test_loss: 5.431433 acc: 0.057625, test_acc: 0.011314\n",
      "20864/20858 Epoch: 180, loss: 4.595981, test_loss: 5.433715 acc: 0.058589, test_acc: 0.009971\n",
      "20864/20858 Epoch: 181, loss: 4.578475, test_loss: 5.442799 acc: 0.057760, test_acc: 0.008437\n",
      "20864/20858 Epoch: 182, loss: 4.582582, test_loss: 5.430377 acc: 0.057043, test_acc: 0.010355\n",
      "20864/20858 Epoch: 183, loss: 4.587552, test_loss: 5.426773 acc: 0.055222, test_acc: 0.009779\n",
      "20864/20858 Epoch: 184, loss: 4.590492, test_loss: 5.416551 acc: 0.054793, test_acc: 0.008821\n",
      "20864/20858 Epoch: 185, loss: 4.576300, test_loss: 5.435025 acc: 0.057676, test_acc: 0.009204\n",
      "20864/20858 Epoch: 186, loss: 4.580768, test_loss: 5.436173 acc: 0.055080, test_acc: 0.009396\n",
      "20864/20858 Epoch: 187, loss: 4.584419, test_loss: 5.446425 acc: 0.058874, test_acc: 0.008821\n",
      "20864/20858 Epoch: 188, loss: 4.569860, test_loss: 5.440412 acc: 0.059933, test_acc: 0.009012\n",
      "20864/20858 Epoch: 189, loss: 4.581109, test_loss: 5.437685 acc: 0.057244, test_acc: 0.009779\n",
      "20864/20858 Epoch: 190, loss: 4.573021, test_loss: 5.461691 acc: 0.058157, test_acc: 0.009012\n",
      "20864/20858 Epoch: 191, loss: 4.578766, test_loss: 5.444880 acc: 0.059739, test_acc: 0.010163\n",
      "20864/20858 Epoch: 192, loss: 4.573211, test_loss: 5.439775 acc: 0.059689, test_acc: 0.009779\n",
      "20864/20858 Epoch: 193, loss: 4.573608, test_loss: 5.441751 acc: 0.059111, test_acc: 0.008437\n",
      "20864/20858 Epoch: 194, loss: 4.564205, test_loss: 5.463391 acc: 0.059344, test_acc: 0.010930\n",
      "20864/20858 Epoch: 195, loss: 4.549977, test_loss: 5.458822 acc: 0.062382, test_acc: 0.009588\n",
      "20864/20858 Epoch: 196, loss: 4.564696, test_loss: 5.439597 acc: 0.058622, test_acc: 0.010163\n",
      "20864/20858 Epoch: 197, loss: 4.559340, test_loss: 5.433942 acc: 0.060503, test_acc: 0.009971\n",
      "20864/20858 Epoch: 198, loss: 4.555584, test_loss: 5.438335 acc: 0.060549, test_acc: 0.009396\n",
      "20864/20858 Epoch: 199, loss: 4.568666, test_loss: 5.441942 acc: 0.059267, test_acc: 0.009204\n",
      "20864/20858 Epoch: 200, loss: 4.558119, test_loss: 5.439523 acc: 0.058399, test_acc: 0.010355\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "for ep in range(200):\n",
    "    all_loss = []\n",
    "    all_ac = []\n",
    "    all_len = len(train_data)\n",
    "    i = 0\n",
    "    for b_data, b_labels, b_data_sizes in get_batch(train_data, train_y, train_l_d, batch_size):\n",
    "        _, batch_loss, batch_ac = sess.run([train_op, loss, accuracy], feed_dict={pages:b_data, labels:b_labels,\n",
    "                                                              pages_lengths:b_data_sizes,\n",
    "                                                              dropout:0.5})\n",
    "        all_loss.append(batch_loss)\n",
    "        all_ac.append(batch_ac)\n",
    "        i += 1\n",
    "        print('\\r%d/%d' % (i*batch_size, all_len), end='')\n",
    "        \n",
    "        \n",
    "    ls, ac = sess.run([loss, accuracy], feed_dict={labels:test_y, pages:test_data, pages_lengths:test_l_d, dropout:1.0})      \n",
    "    \n",
    "    print(' Epoch: %d, loss: %f, test_loss: %f' % (ep+1, np.mean(all_loss), ls), end='')\n",
    "    print(' acc: %f, test_acc: %f' % (np.mean(all_ac), ac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 50\n",
    "embedding_matrix = np.random.uniform(-np.sqrt(3/dim), np.sqrt(3/dim), (len(le.classes_)+1, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = tf.placeholder(tf.int32, shape=[None, MAX_LEN]) # Input data\n",
    "\n",
    "dropout = tf.placeholder(dtype=tf.float32, shape=[]) # Placeholder to set dropout amount\n",
    "\n",
    "W = tf.Variable(embedding_matrix, dtype=tf.float32)\n",
    "embeddings = tf.nn.embedding_lookup(W, pages) # Use embeddings for pages - it allow to use additional info\n",
    "\n",
    "embeddings = tf.nn.dropout(embeddings, dropout) # Add dropout\n",
    "embeddings = tf.reshape(embeddings, [-1, 10, 50, 1])\n",
    "\n",
    "# CNN layers\n",
    "cnn = tf.layers.Conv2D(512, kernel_size=(3, 50), strides=1, padding='valid', activation=tf.nn.tanh, \n",
    "                       kernel_initializer=tf.keras.initializers.glorot_normal())(embeddings)\n",
    "cnn = tf.layers.batch_normalization(cnn) \n",
    "#cnn = tf.nn.dropout(cnn, dropout)\n",
    "cnn = tf.reshape(cnn, [-1, 8, 512])\n",
    "\n",
    "cnn = tf.layers.Conv1D(512, kernel_size=3, strides=1, padding='valid', activation=tf.nn.relu, \n",
    "                             kernel_initializer=tf.keras.initializers.he_normal())(cnn)\n",
    "cnn = tf.layers.batch_normalization(cnn) \n",
    "#cnn = tf.nn.dropout(cnn, dropout)\n",
    "cnn = tf.layers.Conv1D(256, kernel_size=3, strides=1, padding='valid', activation=tf.nn.relu, \n",
    "                             kernel_initializer=tf.keras.initializers.he_normal())(cnn)\n",
    "cnn = tf.layers.batch_normalization(cnn) \n",
    "#cnn = tf.nn.dropout(cnn, dropout)\n",
    "cnn = tf.layers.Conv1D(128, kernel_size=3, strides=1, padding='valid', activation=tf.nn.relu, \n",
    "                             kernel_initializer=tf.keras.initializers.he_normal())(cnn)\n",
    "cnn = tf.layers.batch_normalization(cnn) \n",
    "\n",
    "cnn = tf.layers.MaxPooling1D(pool_size=2, strides=2)(cnn)\n",
    "\n",
    "cnn = tf.layers.Flatten()(cnn)\n",
    "\n",
    "out = tf.layers.dense(cnn, 64, activation=tf.nn.relu, kernel_initializer=tf.keras.initializers.he_normal())\n",
    "#out = tf.nn.dropout(out, dropout) # Add dropout\n",
    "#out = tf.layers.dense(out, 128, activation=tf.nn.relu, kernel_initializer=tf.keras.initializers.he_normal())\n",
    "#out = tf.nn.dropout(out, dropout) # Add dropout\n",
    "\n",
    "# Output layer with number of units equal to number of classes\n",
    "# Use no activation because of softmax_cross_entropy_with_logits\n",
    "out = tf.layers.dense(out, len(le.classes_)) \n",
    "ans = tf.nn.softmax(out) # Add softmax activation for use output of NN\n",
    "\n",
    "labels = tf.placeholder(tf.int32, shape=[None, len(le.classes_)]) # Real next pages for train\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=out))\n",
    "\n",
    "# Compute accuracy\n",
    "correct_predictions = tf.equal(tf.argmax(labels, 1), tf.argmax(ans, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, 'float'))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.001)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20864/20858 Epoch: 1, loss: 5.148835, test_loss: 5.102962 acc: 0.009682, test_acc: 0.011122\n",
      "20864/20858 Epoch: 2, loss: 5.092948, test_loss: 5.103763 acc: 0.010880, test_acc: 0.011505\n",
      "20864/20858 Epoch: 3, loss: 5.090062, test_loss: 5.101890 acc: 0.010741, test_acc: 0.009204\n",
      "20864/20858 Epoch: 4, loss: 5.087626, test_loss: 5.102482 acc: 0.011793, test_acc: 0.011314\n",
      "20864/20858 Epoch: 5, loss: 5.085680, test_loss: 5.101879 acc: 0.011074, test_acc: 0.009396\n",
      "20864/20858 Epoch: 6, loss: 5.083214, test_loss: 5.108016 acc: 0.012517, test_acc: 0.011314\n",
      "20864/20858 Epoch: 7, loss: 5.078872, test_loss: 5.107662 acc: 0.011985, test_acc: 0.011122\n",
      "20864/20858 Epoch: 8, loss: 5.076642, test_loss: 5.112923 acc: 0.012704, test_acc: 0.011505\n",
      "20864/20858 Epoch: 9, loss: 5.072339, test_loss: 5.107238 acc: 0.012462, test_acc: 0.012464\n",
      "20864/20858 Epoch: 10, loss: 5.066676, test_loss: 5.109479 acc: 0.013950, test_acc: 0.013039\n",
      "20864/20858 Epoch: 11, loss: 5.058431, test_loss: 5.116278 acc: 0.015153, test_acc: 0.010547\n",
      "20864/20858 Epoch: 12, loss: 5.054545, test_loss: 5.117511 acc: 0.014571, test_acc: 0.011889\n",
      "20864/20858 Epoch: 13, loss: 5.046742, test_loss: 5.118124 acc: 0.016248, test_acc: 0.012272\n",
      "20864/20858 Epoch: 14, loss: 5.039469, test_loss: 5.127139 acc: 0.017118, test_acc: 0.011889\n",
      "20864/20858 Epoch: 15, loss: 5.030112, test_loss: 5.131443 acc: 0.018026, test_acc: 0.013231\n",
      "20864/20858 Epoch: 16, loss: 5.016491, test_loss: 5.143876 acc: 0.017163, test_acc: 0.013231\n",
      "20864/20858 Epoch: 17, loss: 5.007958, test_loss: 5.149117 acc: 0.018887, test_acc: 0.012081\n",
      "20864/20858 Epoch: 18, loss: 4.992912, test_loss: 5.175188 acc: 0.019129, test_acc: 0.012656\n",
      "20864/20858 Epoch: 19, loss: 4.974081, test_loss: 5.157441 acc: 0.021530, test_acc: 0.012464\n",
      "20864/20858 Epoch: 20, loss: 4.964970, test_loss: 5.185623 acc: 0.023102, test_acc: 0.009588\n",
      "20864/20858 Epoch: 21, loss: 4.946245, test_loss: 5.200220 acc: 0.024971, test_acc: 0.012848\n",
      "20864/20858 Epoch: 22, loss: 4.927071, test_loss: 5.209991 acc: 0.025323, test_acc: 0.010930\n",
      "20864/20858 Epoch: 23, loss: 4.911589, test_loss: 5.225117 acc: 0.027519, test_acc: 0.012848\n",
      "20864/20858 Epoch: 24, loss: 4.890251, test_loss: 5.257200 acc: 0.030298, test_acc: 0.013039\n",
      "20864/20858 Epoch: 25, loss: 4.869453, test_loss: 5.282732 acc: 0.033076, test_acc: 0.011505\n",
      "20864/20858 Epoch: 26, loss: 4.843098, test_loss: 5.281635 acc: 0.034754, test_acc: 0.009396\n",
      "20864/20858 Epoch: 27, loss: 4.820107, test_loss: 5.335601 acc: 0.038072, test_acc: 0.008629\n",
      "20864/20858 Epoch: 28, loss: 4.792483, test_loss: 5.330412 acc: 0.040749, test_acc: 0.009396\n",
      "20864/20858 Epoch: 29, loss: 4.762857, test_loss: 5.377267 acc: 0.044387, test_acc: 0.010355\n",
      "20864/20858 Epoch: 30, loss: 4.741245, test_loss: 5.363936 acc: 0.046599, test_acc: 0.009971\n",
      "20864/20858 Epoch: 31, loss: 4.701752, test_loss: 5.417185 acc: 0.052540, test_acc: 0.009396\n",
      "20864/20858 Epoch: 32, loss: 4.665621, test_loss: 5.442115 acc: 0.055996, test_acc: 0.010930\n",
      "20864/20858 Epoch: 33, loss: 4.634197, test_loss: 5.480902 acc: 0.057379, test_acc: 0.009971\n",
      "20864/20858 Epoch: 34, loss: 4.592583, test_loss: 5.535826 acc: 0.064434, test_acc: 0.009779\n",
      "20864/20858 Epoch: 35, loss: 4.547535, test_loss: 5.534426 acc: 0.068122, test_acc: 0.010163\n",
      "20864/20858 Epoch: 36, loss: 4.511074, test_loss: 5.622777 acc: 0.073969, test_acc: 0.010355\n",
      "20864/20858 Epoch: 37, loss: 4.473242, test_loss: 5.629953 acc: 0.079601, test_acc: 0.009204\n",
      "20864/20858 Epoch: 38, loss: 4.432542, test_loss: 5.683740 acc: 0.086821, test_acc: 0.009588\n",
      "20864/20858 Epoch: 39, loss: 4.386629, test_loss: 5.747278 acc: 0.091519, test_acc: 0.009779\n",
      "20864/20858 Epoch: 40, loss: 4.345632, test_loss: 5.745627 acc: 0.096405, test_acc: 0.009588\n",
      "20864/20858 Epoch: 41, loss: 4.296212, test_loss: 5.805587 acc: 0.106238, test_acc: 0.008821\n",
      "20864/20858 Epoch: 42, loss: 4.251439, test_loss: 5.871183 acc: 0.111380, test_acc: 0.007287\n",
      "20864/20858 Epoch: 43, loss: 4.200734, test_loss: 5.974476 acc: 0.120539, test_acc: 0.007095\n",
      "20864/20858 Epoch: 44, loss: 4.162470, test_loss: 5.999664 acc: 0.127176, test_acc: 0.008629\n",
      "20864/20858 Epoch: 45, loss: 4.107412, test_loss: 6.032212 acc: 0.130348, test_acc: 0.008437\n",
      "20864/20858 Epoch: 46, loss: 4.050504, test_loss: 6.127951 acc: 0.141914, test_acc: 0.008821\n",
      "20864/20858 Epoch: 47, loss: 4.014994, test_loss: 6.104716 acc: 0.143728, test_acc: 0.008821\n",
      "20864/20858 Epoch: 48, loss: 3.949530, test_loss: 6.175983 acc: 0.153832, test_acc: 0.009396\n",
      "20864/20858 Epoch: 49, loss: 3.914971, test_loss: 6.227180 acc: 0.161452, test_acc: 0.008245\n",
      "20864/20858 Epoch: 50, loss: 3.859506, test_loss: 6.314949 acc: 0.166753, test_acc: 0.008629\n",
      "20864/20858 Epoch: 51, loss: 3.810020, test_loss: 6.424683 acc: 0.177530, test_acc: 0.009396\n",
      "20864/20858 Epoch: 52, loss: 3.763001, test_loss: 6.519610 acc: 0.185041, test_acc: 0.008245\n",
      "20864/20858 Epoch: 53, loss: 3.711236, test_loss: 6.511056 acc: 0.193454, test_acc: 0.007862\n",
      "20864/20858 Epoch: 54, loss: 3.658451, test_loss: 6.557313 acc: 0.201904, test_acc: 0.008245\n",
      "20864/20858 Epoch: 55, loss: 3.613570, test_loss: 6.642589 acc: 0.212446, test_acc: 0.007095\n",
      "20864/20858 Epoch: 56, loss: 3.541679, test_loss: 6.699832 acc: 0.222708, test_acc: 0.008629\n",
      "20864/20858 Epoch: 57, loss: 3.506004, test_loss: 6.768954 acc: 0.229929, test_acc: 0.008437\n",
      "20864/20858 Epoch: 58, loss: 3.458907, test_loss: 6.944144 acc: 0.234199, test_acc: 0.010547\n",
      "20864/20858 Epoch: 59, loss: 3.401896, test_loss: 6.963326 acc: 0.241966, test_acc: 0.007095\n",
      "20864/20858 Epoch: 60, loss: 3.365147, test_loss: 7.052210 acc: 0.253601, test_acc: 0.007478\n",
      "20864/20858 Epoch: 61, loss: 3.308711, test_loss: 7.217272 acc: 0.262514, test_acc: 0.007862\n",
      "20864/20858 Epoch: 62, loss: 3.255940, test_loss: 7.173336 acc: 0.273036, test_acc: 0.009396\n",
      "20864/20858 Epoch: 63, loss: 3.213997, test_loss: 7.281158 acc: 0.281014, test_acc: 0.008821\n",
      "20864/20858 Epoch: 64, loss: 3.175657, test_loss: 7.308011 acc: 0.284259, test_acc: 0.009588\n",
      "20864/20858 Epoch: 65, loss: 3.116593, test_loss: 7.463968 acc: 0.289605, test_acc: 0.009588\n",
      "20864/20858 Epoch: 66, loss: 3.075253, test_loss: 7.491666 acc: 0.303848, test_acc: 0.007478\n",
      "20864/20858 Epoch: 67, loss: 3.028651, test_loss: 7.698908 acc: 0.312350, test_acc: 0.009779\n",
      "20864/20858 Epoch: 68, loss: 2.986758, test_loss: 7.739275 acc: 0.319321, test_acc: 0.008629\n",
      "20864/20858 Epoch: 69, loss: 2.936193, test_loss: 7.747436 acc: 0.331586, test_acc: 0.007862\n",
      "20864/20858 Epoch: 70, loss: 2.873121, test_loss: 7.822319 acc: 0.341024, test_acc: 0.008821\n",
      "20864/20858 Epoch: 71, loss: 2.849038, test_loss: 8.010723 acc: 0.345795, test_acc: 0.008437\n",
      "20864/20858 Epoch: 72, loss: 2.803627, test_loss: 8.077525 acc: 0.355651, test_acc: 0.009012\n",
      "20864/20858 Epoch: 73, loss: 2.737414, test_loss: 8.315400 acc: 0.368775, test_acc: 0.008437\n",
      "20864/20858 Epoch: 74, loss: 2.716400, test_loss: 8.230786 acc: 0.374581, test_acc: 0.007478\n",
      "20864/20858 Epoch: 75, loss: 2.671761, test_loss: 8.443867 acc: 0.384114, test_acc: 0.008821\n",
      "20864/20858 Epoch: 76, loss: 2.605473, test_loss: 8.484650 acc: 0.397525, test_acc: 0.009588\n",
      "20864/20858 Epoch: 77, loss: 2.568676, test_loss: 8.484211 acc: 0.403346, test_acc: 0.009012\n",
      "20864/20858 Epoch: 78, loss: 2.530566, test_loss: 8.726389 acc: 0.411298, test_acc: 0.008821\n",
      "20864/20858 Epoch: 79, loss: 2.484088, test_loss: 9.012887 acc: 0.419122, test_acc: 0.009971\n",
      "20864/20858 Epoch: 80, loss: 2.459250, test_loss: 8.979478 acc: 0.428600, test_acc: 0.008054\n",
      "20864/20858 Epoch: 81, loss: 2.386924, test_loss: 9.052967 acc: 0.441277, test_acc: 0.006711\n",
      "20864/20858 Epoch: 82, loss: 2.352007, test_loss: 9.211034 acc: 0.443470, test_acc: 0.006136\n",
      "20864/20858 Epoch: 83, loss: 2.325553, test_loss: 9.377452 acc: 0.456418, test_acc: 0.007287\n",
      "20864/20858 Epoch: 84, loss: 2.276907, test_loss: 9.344731 acc: 0.463361, test_acc: 0.007478\n",
      "20864/20858 Epoch: 85, loss: 2.239053, test_loss: 9.472000 acc: 0.474499, test_acc: 0.008245\n",
      "20864/20858 Epoch: 86, loss: 2.190662, test_loss: 9.729998 acc: 0.483893, test_acc: 0.009588\n",
      "20864/20858 Epoch: 87, loss: 2.163371, test_loss: 9.663379 acc: 0.489674, test_acc: 0.007478\n",
      "20864/20858 Epoch: 88, loss: 2.133548, test_loss: 9.792048 acc: 0.496944, test_acc: 0.007095\n",
      "20864/20858 Epoch: 89, loss: 2.081406, test_loss: 9.979765 acc: 0.509279, test_acc: 0.006520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20864/20858 Epoch: 90, loss: 2.044392, test_loss: 10.065938 acc: 0.516823, test_acc: 0.008245\n",
      "20864/20858 Epoch: 91, loss: 2.007464, test_loss: 10.457519 acc: 0.522824, test_acc: 0.007862\n",
      "20864/20858 Epoch: 92, loss: 1.987570, test_loss: 10.571976 acc: 0.527658, test_acc: 0.009396\n",
      "20864/20858 Epoch: 93, loss: 1.929759, test_loss: 10.648362 acc: 0.542355, test_acc: 0.006328\n",
      "20864/20858 Epoch: 94, loss: 1.918874, test_loss: 10.930301 acc: 0.542626, test_acc: 0.007478\n",
      "20864/20858 Epoch: 95, loss: 1.884022, test_loss: 10.830804 acc: 0.549132, test_acc: 0.007478\n",
      "20864/20858 Epoch: 96, loss: 1.834244, test_loss: 11.028135 acc: 0.563576, test_acc: 0.008437\n",
      "20864/20858 Epoch: 97, loss: 1.814580, test_loss: 11.058257 acc: 0.570243, test_acc: 0.010547\n",
      "20864/20858 Epoch: 98, loss: 1.791097, test_loss: 11.183058 acc: 0.574583, test_acc: 0.006136\n",
      "20864/20858 Epoch: 99, loss: 1.775596, test_loss: 11.428587 acc: 0.579529, test_acc: 0.007862\n",
      "20864/20858 Epoch: 100, loss: 1.737288, test_loss: 11.267939 acc: 0.588098, test_acc: 0.006711\n",
      "20864/20858 Epoch: 101, loss: 1.696727, test_loss: 11.720623 acc: 0.595892, test_acc: 0.007862\n",
      "20864/20858 Epoch: 102, loss: 1.682692, test_loss: 11.591717 acc: 0.600470, test_acc: 0.007670\n",
      "20864/20858 Epoch: 103, loss: 1.647366, test_loss: 11.818003 acc: 0.613201, test_acc: 0.007670\n",
      "20864/20858 Epoch: 104, loss: 1.641577, test_loss: 11.727433 acc: 0.611361, test_acc: 0.006328\n",
      "20864/20858 Epoch: 105, loss: 1.623221, test_loss: 12.099635 acc: 0.615495, test_acc: 0.008245\n",
      "20864/20858 Epoch: 106, loss: 1.580073, test_loss: 12.194117 acc: 0.624115, test_acc: 0.008821\n",
      "20864/20858 Epoch: 107, loss: 1.568760, test_loss: 12.185257 acc: 0.626916, test_acc: 0.007670\n",
      "20864/20858 Epoch: 108, loss: 1.528930, test_loss: 12.471969 acc: 0.642453, test_acc: 0.007862\n",
      "20864/20858 Epoch: 109, loss: 1.529340, test_loss: 12.517890 acc: 0.635427, test_acc: 0.009396\n",
      "20864/20858 Epoch: 110, loss: 1.518345, test_loss: 12.566432 acc: 0.640284, test_acc: 0.007095\n",
      "20864/20858 Epoch: 111, loss: 1.460907, test_loss: 12.840075 acc: 0.653462, test_acc: 0.008054\n",
      "20864/20858 Epoch: 112, loss: 1.458320, test_loss: 12.825024 acc: 0.656959, test_acc: 0.009971\n",
      "20864/20858 Epoch: 113, loss: 1.463385, test_loss: 12.830919 acc: 0.651921, test_acc: 0.006903\n",
      "20864/20858 Epoch: 114, loss: 1.422345, test_loss: 13.139143 acc: 0.662614, test_acc: 0.005369\n",
      "20864/20858 Epoch: 115, loss: 1.404196, test_loss: 13.189996 acc: 0.668948, test_acc: 0.008437\n",
      "20864/20858 Epoch: 116, loss: 1.386417, test_loss: 13.319366 acc: 0.671895, test_acc: 0.006903\n",
      "20864/20858 Epoch: 117, loss: 1.388428, test_loss: 13.243073 acc: 0.672152, test_acc: 0.006520\n",
      "20864/20858 Epoch: 118, loss: 1.352303, test_loss: 13.616781 acc: 0.683346, test_acc: 0.006903\n",
      "20864/20858 Epoch: 119, loss: 1.331970, test_loss: 13.711077 acc: 0.687928, test_acc: 0.006520\n",
      "20864/20858 Epoch: 120, loss: 1.336673, test_loss: 13.885073 acc: 0.685822, test_acc: 0.007670\n",
      "20864/20858 Epoch: 121, loss: 1.344010, test_loss: 13.715709 acc: 0.686495, test_acc: 0.007287\n",
      "20864/20858 Epoch: 122, loss: 1.312883, test_loss: 13.826915 acc: 0.689273, test_acc: 0.007478\n",
      "20864/20858 Epoch: 123, loss: 1.298766, test_loss: 14.020756 acc: 0.693629, test_acc: 0.007287\n",
      "20864/20858 Epoch: 124, loss: 1.260494, test_loss: 14.216268 acc: 0.702264, test_acc: 0.006520\n",
      "20864/20858 Epoch: 125, loss: 1.240207, test_loss: 14.110617 acc: 0.708097, test_acc: 0.004986\n",
      "20864/20858 Epoch: 126, loss: 1.262298, test_loss: 14.116774 acc: 0.701557, test_acc: 0.006711\n",
      "20864/20858 Epoch: 127, loss: 1.220486, test_loss: 14.319352 acc: 0.713192, test_acc: 0.006903\n",
      "20864/20858 Epoch: 128, loss: 1.229321, test_loss: 14.162412 acc: 0.709140, test_acc: 0.007095\n",
      "20864/20858 Epoch: 129, loss: 1.209668, test_loss: 14.364726 acc: 0.715579, test_acc: 0.005753\n",
      "20864/20858 Epoch: 130, loss: 1.207260, test_loss: 14.490072 acc: 0.716372, test_acc: 0.006711\n",
      "20864/20858 Epoch: 131, loss: 1.173506, test_loss: 14.709599 acc: 0.724333, test_acc: 0.005561\n",
      "20864/20858 Epoch: 132, loss: 1.185870, test_loss: 14.534321 acc: 0.723185, test_acc: 0.007287\n",
      "20864/20858 Epoch: 133, loss: 1.180228, test_loss: 14.909900 acc: 0.722257, test_acc: 0.006328\n",
      "20864/20858 Epoch: 134, loss: 1.163531, test_loss: 14.799963 acc: 0.726430, test_acc: 0.006136\n",
      "20864/20858 Epoch: 135, loss: 1.143739, test_loss: 14.836185 acc: 0.736150, test_acc: 0.007287\n",
      "20864/20858 Epoch: 136, loss: 1.123097, test_loss: 14.994382 acc: 0.739538, test_acc: 0.007670\n",
      "20864/20858 Epoch: 137, loss: 1.135875, test_loss: 15.070544 acc: 0.736720, test_acc: 0.006136\n",
      "20864/20858 Epoch: 138, loss: 1.119030, test_loss: 15.250176 acc: 0.737451, test_acc: 0.006520\n",
      "20864/20858 Epoch: 139, loss: 1.100365, test_loss: 15.368251 acc: 0.743883, test_acc: 0.005944\n",
      "20864/20858 Epoch: 140, loss: 1.110060, test_loss: 15.466281 acc: 0.740916, test_acc: 0.006711\n",
      "20864/20858 Epoch: 141, loss: 1.099736, test_loss: 15.439909 acc: 0.743298, test_acc: 0.006136\n",
      "20864/20858 Epoch: 142, loss: 1.098921, test_loss: 15.403846 acc: 0.741982, test_acc: 0.005944\n",
      "20864/20858 Epoch: 143, loss: 1.073778, test_loss: 15.576446 acc: 0.749199, test_acc: 0.005369\n",
      "20864/20858 Epoch: 144, loss: 1.063809, test_loss: 15.674730 acc: 0.753136, test_acc: 0.005561\n",
      "20864/20858 Epoch: 145, loss: 1.064609, test_loss: 15.478840 acc: 0.754657, test_acc: 0.006903\n",
      "20864/20858 Epoch: 146, loss: 1.042207, test_loss: 15.751323 acc: 0.759474, test_acc: 0.005369\n",
      "20864/20858 Epoch: 147, loss: 1.043973, test_loss: 15.983953 acc: 0.759034, test_acc: 0.004410\n",
      "20864/20858 Epoch: 148, loss: 1.041034, test_loss: 15.932118 acc: 0.758094, test_acc: 0.007862\n",
      "20864/20858 Epoch: 149, loss: 1.036981, test_loss: 16.186634 acc: 0.759362, test_acc: 0.006328\n",
      "20864/20858 Epoch: 150, loss: 1.024099, test_loss: 15.986018 acc: 0.762808, test_acc: 0.007095\n",
      "20864/20858 Epoch: 151, loss: 1.035631, test_loss: 15.948412 acc: 0.759789, test_acc: 0.005561\n",
      "20864/20858 Epoch: 152, loss: 1.009828, test_loss: 16.129900 acc: 0.766556, test_acc: 0.004602\n",
      "20864/20858 Epoch: 153, loss: 1.003892, test_loss: 16.053566 acc: 0.768443, test_acc: 0.005177\n",
      "20864/20858 Epoch: 154, loss: 0.992891, test_loss: 16.343155 acc: 0.770505, test_acc: 0.007287\n",
      "20864/20858 Epoch: 155, loss: 0.977290, test_loss: 16.691868 acc: 0.774773, test_acc: 0.006711\n",
      "20864/20858 Epoch: 156, loss: 1.002407, test_loss: 16.233034 acc: 0.769923, test_acc: 0.007287\n",
      "20864/20858 Epoch: 157, loss: 0.989778, test_loss: 16.335085 acc: 0.772353, test_acc: 0.005753\n",
      "20864/20858 Epoch: 158, loss: 0.967114, test_loss: 16.419498 acc: 0.776432, test_acc: 0.006136\n",
      "20864/20858 Epoch: 159, loss: 0.960987, test_loss: 16.659351 acc: 0.779178, test_acc: 0.005753\n",
      "20864/20858 Epoch: 160, loss: 0.962201, test_loss: 16.719336 acc: 0.777393, test_acc: 0.007478\n",
      "20864/20858 Epoch: 161, loss: 0.957930, test_loss: 16.442486 acc: 0.779303, test_acc: 0.008054\n",
      "20864/20858 Epoch: 162, loss: 0.959104, test_loss: 16.365891 acc: 0.779025, test_acc: 0.007478\n",
      "20864/20858 Epoch: 163, loss: 0.931934, test_loss: 16.546022 acc: 0.783254, test_acc: 0.005753\n",
      "20864/20858 Epoch: 164, loss: 0.932545, test_loss: 16.617371 acc: 0.784966, test_acc: 0.007670\n",
      "20864/20858 Epoch: 165, loss: 0.920457, test_loss: 16.888515 acc: 0.788762, test_acc: 0.007478\n",
      "20864/20858 Epoch: 166, loss: 0.922620, test_loss: 17.008724 acc: 0.787204, test_acc: 0.007670\n",
      "20864/20858 Epoch: 167, loss: 0.942692, test_loss: 16.907612 acc: 0.782250, test_acc: 0.008437\n",
      "20864/20858 Epoch: 168, loss: 0.919697, test_loss: 17.158846 acc: 0.785570, test_acc: 0.008629\n",
      "20864/20858 Epoch: 169, loss: 0.913400, test_loss: 16.921976 acc: 0.788919, test_acc: 0.009204\n",
      "20864/20858 Epoch: 170, loss: 0.903641, test_loss: 16.703817 acc: 0.794173, test_acc: 0.006328\n",
      "20864/20858 Epoch: 171, loss: 0.896045, test_loss: 16.815935 acc: 0.794885, test_acc: 0.007862\n",
      "20864/20858 Epoch: 172, loss: 0.902291, test_loss: 16.804197 acc: 0.791331, test_acc: 0.007862\n",
      "20864/20858 Epoch: 173, loss: 0.889193, test_loss: 17.253611 acc: 0.794588, test_acc: 0.005944\n",
      "20864/20858 Epoch: 174, loss: 0.878919, test_loss: 17.655069 acc: 0.796675, test_acc: 0.007095\n",
      "20864/20858 Epoch: 175, loss: 0.902104, test_loss: 17.279320 acc: 0.792110, test_acc: 0.006903\n",
      "20864/20858 Epoch: 176, loss: 0.874293, test_loss: 17.029083 acc: 0.799107, test_acc: 0.007095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20864/20858 Epoch: 177, loss: 0.871556, test_loss: 17.288116 acc: 0.800696, test_acc: 0.007478\n",
      "20864/20858 Epoch: 178, loss: 0.845267, test_loss: 17.602947 acc: 0.804087, test_acc: 0.007478\n",
      "20864/20858 Epoch: 179, loss: 0.853503, test_loss: 17.592316 acc: 0.802225, test_acc: 0.007095\n",
      "20864/20858 Epoch: 180, loss: 0.851883, test_loss: 17.587978 acc: 0.802040, test_acc: 0.004986\n",
      "20864/20858 Epoch: 181, loss: 0.846989, test_loss: 17.785667 acc: 0.803214, test_acc: 0.006136\n",
      "20864/20858 Epoch: 182, loss: 0.857224, test_loss: 17.377731 acc: 0.802417, test_acc: 0.006520\n",
      "20864/20858 Epoch: 183, loss: 0.849758, test_loss: 17.310322 acc: 0.805788, test_acc: 0.007287\n",
      "20864/20858 Epoch: 184, loss: 0.834519, test_loss: 17.532349 acc: 0.809714, test_acc: 0.005561\n",
      "20864/20858 Epoch: 185, loss: 0.831803, test_loss: 17.934048 acc: 0.807502, test_acc: 0.006711\n",
      "20864/20858 Epoch: 186, loss: 0.835229, test_loss: 18.109716 acc: 0.806551, test_acc: 0.007862\n",
      "20864/20858 Epoch: 187, loss: 0.824462, test_loss: 17.765987 acc: 0.812113, test_acc: 0.006520\n",
      "20864/20858 Epoch: 188, loss: 0.826513, test_loss: 17.839935 acc: 0.809273, test_acc: 0.005944\n",
      "20864/20858 Epoch: 189, loss: 0.823644, test_loss: 17.986244 acc: 0.808299, test_acc: 0.006136\n",
      "20864/20858 Epoch: 190, loss: 0.818343, test_loss: 18.053164 acc: 0.812635, test_acc: 0.006903\n",
      "20864/20858 Epoch: 191, loss: 0.813927, test_loss: 17.667835 acc: 0.811188, test_acc: 0.007478\n",
      "20864/20858 Epoch: 192, loss: 0.807709, test_loss: 18.070553 acc: 0.814461, test_acc: 0.006711\n",
      "20864/20858 Epoch: 193, loss: 0.812342, test_loss: 17.887531 acc: 0.810845, test_acc: 0.006711\n",
      "20864/20858 Epoch: 194, loss: 0.806692, test_loss: 18.155615 acc: 0.812985, test_acc: 0.006520\n",
      "20864/20858 Epoch: 195, loss: 0.798691, test_loss: 18.115841 acc: 0.816846, test_acc: 0.008437\n",
      "20864/20858 Epoch: 196, loss: 0.788131, test_loss: 18.189112 acc: 0.817615, test_acc: 0.008054\n",
      "20864/20858 Epoch: 197, loss: 0.789622, test_loss: 18.444757 acc: 0.816196, test_acc: 0.007095\n",
      "20864/20858 Epoch: 198, loss: 0.786422, test_loss: 18.208094 acc: 0.818296, test_acc: 0.009588\n",
      "20864/20858 Epoch: 199, loss: 0.785564, test_loss: 18.451313 acc: 0.818725, test_acc: 0.008054\n",
      "20864/20858 Epoch: 200, loss: 0.772943, test_loss: 18.302721 acc: 0.820352, test_acc: 0.009588\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "for ep in range(200):\n",
    "    all_loss = []\n",
    "    all_ac = []\n",
    "    all_len = len(train_data)\n",
    "    i = 0\n",
    "    for b_data, b_labels, b_data_sizes in get_batch(train_data, train_y, train_l_d, batch_size):\n",
    "        _, batch_loss, batch_ac = sess.run([train_op, loss, accuracy], feed_dict={pages:b_data, labels:b_labels,\n",
    "                                                                                  dropout:0.5})\n",
    "        all_loss.append(batch_loss)\n",
    "        all_ac.append(batch_ac)\n",
    "        i += 1\n",
    "        print('\\r%d/%d' % (i*batch_size, all_len), end='')\n",
    "        \n",
    "        \n",
    "    ls, ac = sess.run([loss, accuracy], feed_dict={pages:test_data, labels:test_y, dropout:1.0})      \n",
    "    \n",
    "    print(' Epoch: %d, loss: %f, test_loss: %f' % (ep+1, np.mean(all_loss), ls), end='')\n",
    "    print(' acc: %f, test_acc: %f' % (np.mean(all_ac), ac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf.fit(train_data, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.801375"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = rf.predict(train_data)\n",
    "acc(train_y, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7115"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = rf.predict(test_data)\n",
    "acc(test_y, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "gb.fit(train_data, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = gb.predict(train_data)\n",
    "acc(train_y, res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-74c2043a295f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "print(classification_report(res, train_y, target_names=list(le.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7115"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = gb.predict(test_data)\n",
    "acc(test_y, res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  precision    recall  f1-score   support\n",
      "\n",
      "                                                                                      /ShopSmall       0.00      0.00      0.00         0\n",
      "                                                                             /account-management       0.00      0.00      0.00         0\n",
      "                                                                         /account-management/PIN       0.03      0.25      0.05        12\n",
      "                                                            /account-management/card-replacement       0.00      0.00      0.00         0\n",
      "                                                           /account-management/card-replacement/       0.00      0.00      0.00         0\n",
      "                                                        /account-management/check-spending-power       0.00      0.00      0.00         0\n",
      "                                                             /account-management/digital-wallets       0.00      0.00      0.00         0\n",
      "                                                                       /account-management/error       0.00      0.00      0.00         0\n",
      "                                                                         /account-management/pin       0.00      0.00      0.00         0\n",
      "                                                                  /account-management/pin/change       0.00      0.00      0.00         0\n",
      "                                                                                       /accounts       0.00      0.00      0.00         0\n",
      "                                                                                 /accounts/error       0.00      0.00      0.00         0\n",
      "                                                                                       /activity       0.00      0.00      0.00         0\n",
      "                                                                                   /address/edit       0.00      0.00      0.00         0\n",
      "                                                               /authentication/recovery/password       0.00      0.00      0.00         0\n",
      "                                                                 /authentication/recovery/userid       0.00      0.00      0.00         0\n",
      "                                                            /card-benefits/detail/2x-gyro-travel       0.00      0.00      0.00         1\n",
      "                                                              /card-benefits/detail/OPEN-Savings       0.00      0.00      0.00         0\n",
      "                                             /card-benefits/detail/account-manager/open-benefits       0.00      0.00      0.00         0\n",
      "                                                        /card-benefits/detail/airline-fee-credit       0.00      0.00      0.00         0\n",
      "                                          /card-benefits/detail/airline-fee-credit/hilton-aspire       0.00      0.00      0.00         0\n",
      "                                                   /card-benefits/detail/amazon-shop-with-points       0.00      0.00      0.00         0\n",
      "                                                     /card-benefits/detail/auto-purchase-program       0.00      0.00      0.00         0\n",
      "                                              /card-benefits/detail/baggage-insurance-plan-basic       0.00      0.00      0.00         0\n",
      "                                                    /card-benefits/detail/blue-cash-everyday-321       0.00      0.00      0.00         0\n",
      "                                                   /card-benefits/detail/blue-cash-preferred-631       0.00      0.00      0.00         0\n",
      "                                                          /card-benefits/detail/business-rewards       0.00      0.00      0.00         0\n",
      "                                                      /card-benefits/detail/car-rental-insurance       0.00      0.00      0.00         0\n",
      "                                                           /card-benefits/detail/digital-wallets       0.00      0.00      0.00         0\n",
      "                                                        /card-benefits/detail/everyday-purchases       0.00      0.00      0.00         0\n",
      "                                                             /card-benefits/detail/global-assist       0.00      0.00      0.00         0\n",
      "                                                               /card-benefits/detail/gold-status       0.00      0.00      0.00         0\n",
      "                                                 /card-benefits/detail/gold-status/hilton-honors       0.00      0.00      0.00         0\n",
      "                                                 /card-benefits/detail/gyro-everyday-spend-bonus       0.00      0.00      0.00         0\n",
      "                                                               /card-benefits/detail/gyro-offers       0.00      0.00      0.00         0\n",
      "                                             /card-benefits/detail/hilton-honors-bonus-points-hs       0.00      0.00      0.00         0\n",
      "                                                    /card-benefits/detail/membership-experiences       0.00      0.00      0.00         0\n",
      "                                                        /card-benefits/detail/membership-rewards       0.00      0.00      0.00         0\n",
      "                                          /card-benefits/detail/membership-rewards/gyro-everyday       0.00      0.00      0.00         0\n",
      "                                               /card-benefits/detail/no-foreign-transaction-fees       0.00      0.00      0.00         0\n",
      "                                                  /card-benefits/detail/no-fx-fees/hilton-honors       0.00      0.00      0.00         0\n",
      "                                                       /card-benefits/detail/points-money-slider       0.00      0.00      0.00         0\n",
      "                                                      /card-benefits/detail/priority-pass-select       0.00      0.00      0.00         0\n",
      "                                                     /card-benefits/detail/priority-pass-select/       0.00      0.00      0.00         0\n",
      "                                         /card-benefits/detail/priority-pass-select/hilton-ascen       0.00      0.00      0.00         0\n",
      "                                                    /card-benefits/detail/redeem-blue-sky-points       0.00      0.00      0.00         0\n",
      "                                                         /card-benefits/detail/redeem-starpoints       0.00      0.00      0.00         0\n",
      "                                               /card-benefits/detail/roadside-assistance-hotline       0.00      0.00      0.00         0\n",
      "                                                                /card-benefits/detail/shoprunner       0.00      0.00      0.00         0\n",
      "                                                     /card-benefits/detail/the-lounge-collection       0.00      0.00      0.00         0\n",
      "                                                 /card-benefits/detail/travel-accident-insurance       0.00      0.00      0.00         0\n",
      "                                          /card-benefits/enroll/airline-fee-credit/hilton-aspire       0.00      0.00      0.00         0\n",
      "                                                                         /card-benefits/view-all       0.00      0.00      0.00         0\n",
      "                                                                        /card-benefits/view-all/       0.04      0.33      0.07         3\n",
      "                                                                    /card-benefits/view-all/blue       0.00      0.00      0.00         1\n",
      "                                                      /card-benefits/view-all/blue-cash-everyday       0.00      0.00      0.00         0\n",
      "                                                     /card-benefits/view-all/blue-cash-preferred       0.00      0.00      0.00         0\n",
      "                                                                   /card-benefits/view-all/error       0.00      0.00      0.00         0\n",
      "                                                           /card-benefits/view-all/gyro-everyday       0.00      0.00      0.00         0\n",
      "                                                 /card-benefits/view-all/gyro-everyday-preferred       0.00      0.00      0.00         0\n",
      "                                                     /card-benefits/view-all/gyro-everyday/error       0.00      0.00      0.00         0\n",
      "                                                            /card-benefits/view-all/gyro-generic       0.00      0.00      0.00         0\n",
      "                                                           /card-benefits/view-all/hilton-honors       0.00      0.00      0.00         0\n",
      "                                                           /card-benefits/view-all/open-benefits       0.00      0.00      0.00         0\n",
      "                                                     /card-benefits/view-all/open-benefits/error       0.00      0.00      0.00         0\n",
      "                                                                  /card-benefits/view-all/plenti       0.00      0.00      0.00         0\n",
      "                                                                /card-benefits/view-all/starwood       0.00      0.00      0.00         0\n",
      "                                                                                     /contact-us       0.00      0.00      0.00         0\n",
      "                                                                                      /dashboard       0.00      0.00      0.00         0\n",
      "                      /dashboard&omnlogin=AU-accountservices-ForgotUserIDPassword-AccountSummary       0.89      0.85      0.87       659\n",
      "                                                     /dashboard&omnlogin=CA_enterpriselogin_myca       0.00      0.00      0.00         0\n",
      "                                                                                     /dashboard/       0.00      0.00      0.00         0\n",
      "                                                                                /dashboard/error       0.00      0.00      0.00         0\n",
      "                                                                                  /direct-debits       0.00      0.00      0.00         2\n",
      "                                                                             /direct-debits/edit       0.00      0.00      0.00         0\n",
      "                                                                           /direct-debits/enroll       0.00      0.00      0.00         0\n",
      "                                                                                     /en-US/maps       0.00      0.00      0.00         0\n",
      "                                                                 /en-US/maps/guides/Chicago-Eats       0.00      0.00      0.00         0\n",
      "                                                               /en-US/maps/guides/San-Diego-Eats       0.00      0.00      0.00         0\n",
      "                                            /en-US/maps/guides/The-Shop-Small-Guide-to-Nashville       0.00      0.00      0.00         0\n",
      "                                                                             /entertainment/home       0.00      0.00      0.00         0\n",
      "                                                                        /entertainment/inventory       0.00      0.00      0.00         0\n",
      "                                                                          /entertainment/listing       0.00      0.00      0.00         0\n",
      "                                                                            /entertainment/login       0.00      0.00      0.00         0\n",
      "                                                                           /entertainment/review       0.00      0.00      0.00         0\n",
      "                                                                                          /error       0.00      0.00      0.00         0\n",
      "                                                                                            /faq       0.00      0.00      0.00         0\n",
      "                                                                                      /faq/error       0.00      0.00      0.00         1\n",
      "                                                                                           /help       0.00      0.00      0.00         0\n",
      "                                                                            /lending/dipp/create       0.00      0.00      0.00         0\n",
      "                                                                     /lending/dipp/create/amount       0.00      0.00      0.00         0\n",
      "                                                                   /lending/dipp/create/complete       0.00      0.00      0.00         0\n",
      "                                                                       /lending/dipp/create/view       0.00      0.00      0.00         0\n",
      "                                                                    /lending/dipp/create/warning       0.00      0.00      0.00         0\n",
      "                                                                              /lending/dipp/exit       0.00      0.00      0.00         0\n",
      "                                                                             /lending/dipp/learn       0.00      0.00      0.00         0\n",
      "                                                                              /lending/dipp/view       0.00      0.00      0.00         0\n",
      "                                                                         /lending/dipp/view/plan       0.00      0.00      0.00         0\n",
      "                                                        /lending/extended-payment-options/enroll       0.00      0.00      0.00         0\n",
      "                                                         /lending/extended-payment-options/offer       0.00      0.00      0.00         0\n",
      "                                                        /lending/extended-payment-options/select       0.00      0.00      0.00         0\n",
      "                                                                    /lending/installments/cancel       0.00      0.00      0.00         0\n",
      "                                                                  /lending/installments/complete       0.00      0.00      0.00         0\n",
      "                                                                    /lending/installments/create       0.00      0.00      0.00         0\n",
      "                                                               /lending/installments/create/list       0.00      0.00      0.00         0\n",
      "                                                               /lending/installments/create/view       0.00      0.00      0.00         1\n",
      "                                                                     /lending/installments/learn       0.00      0.00      0.00         0\n",
      "                                                                      /lending/installments/view       0.00      0.00      0.00         0\n",
      "                                                                 /lending/installments/view/list       0.00      0.00      0.00         0\n",
      "                                                                 /lending/installments/view/plan       0.00      0.00      0.00         0\n",
      "                                                                        /lending/lending-options       0.00      0.00      0.00         0\n",
      "                                                                          /lending/line-increase       0.00      0.00      0.00         0\n",
      "                                                                          /lending/pay-over-time       0.00      0.00      0.00         0\n",
      "                                                                    /lending/pay-your-way/cancel       0.00      0.00      0.00         0\n",
      "                                                                    /lending/pay-your-way/period       0.00      0.00      0.00         0\n",
      "                                                             /lending/pay-your-way/period/select       0.00      0.00      0.00         1\n",
      "                                                     /lending/pay-your-way/period/select/confirm       0.00      0.00      0.00         0\n",
      "                                                               /lending/pay-your-way/submit-fail       0.00      0.00      0.00         0\n",
      "                                                                  /lending/pay-your-way/thankyou       0.00      0.00      0.00         0\n",
      "                                                                                          /login       0.50      0.50      0.50         2\n",
      "                                                                                         /login/       0.03      0.14      0.05         7\n",
      "                                                                                    /login/en-CA       0.00      0.00      0.00         0\n",
      "                                                                                    /login/en-HK       0.00      0.00      0.00         0\n",
      "                                                                              /login/en-HK/error       0.14      0.50      0.22         2\n",
      "                                                                                    /login/en-IN       0.00      0.00      0.00         0\n",
      "                                                                                    /login/en-SG       0.09      0.50      0.15         8\n",
      "                                                                                    /login/en-US       0.00      0.00      0.00         0\n",
      "                                                                              /login/en-US/error       0.93      0.77      0.84      1103\n",
      "                                                                                    /login/error       0.00      0.00      0.00         0\n",
      "                                                                                /login/error/404       0.00      0.00      0.00         0\n",
      "                                                                                    /login/es-ES       0.00      0.00      0.00         0\n",
      "                                                                                    /login/es-MX       0.00      0.00      0.00         0\n",
      "                                                                                    /login/zh-HK       0.00      0.00      0.00         0\n",
      "                                                                              /login/zh-HK/error       0.00      0.00      0.00         0\n",
      "                                                    /lounge-access/hilton-honors-ascend-business       0.00      0.00      0.00         0\n",
      "                                                /lounge-access/hilton-honors-ascend-business/BOS       0.00      0.00      0.00         0\n",
      "                                                /lounge-access/hilton-honors-ascend-business/IAH       0.00      0.00      0.00         0\n",
      "                                                /lounge-access/hilton-honors-ascend-business/LAS       0.00      0.00      0.00         0\n",
      "         /lounge-access/hilton-honors-ascend-business/LAS/The-Club-at-LAS-Concourse-D-X373VPR4Ou       0.00      0.00      0.00         0\n",
      "                                                /lounge-access/hilton-honors-ascend-business/LAX       0.00      0.00      0.00         0\n",
      "/lounge-access/hilton-honors-ascend-business/LAX/Virgin-Atlantic-Clubhouse-Terminal-2-0vQ79n6O7E       0.00      0.00      0.00         0\n",
      "                                                                /lounge-access/the-platinum-card       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/AMS       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/BOS       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/CAN       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/CPH       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/CSX       0.00      0.00      0.00         0\n",
      "    /lounge-access/the-platinum-card/CSX/Joyflight-V4-Lounge-Terminal-2-International-MlbE3OboeD       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/DFW       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/DTW       0.00      0.00      0.00         2\n",
      "                                                            /lounge-access/the-platinum-card/DUB       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/DXB       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/EWR       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/FLL       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/FLR       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/FRA       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/HKG       0.00      0.00      0.00         0\n",
      "                 /lounge-access/the-platinum-card/HKG/The-Centurion-Lounge-Terminal-1-iPvFFfgoeE       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/IAD       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/JFK       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/KIX       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/LAX       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/LGA       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/MCO       0.00      0.00      0.00         0\n",
      "                      /lounge-access/the-platinum-card/MCO/The-Club-at-MCO-Terminal-A-2wEXtxtGAX       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/MEX       0.00      0.00      0.00         1\n",
      "                                                            /lounge-access/the-platinum-card/MIA       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/ORD       0.00      0.00      0.00         0\n",
      "    /lounge-access/the-platinum-card/ORD/Air-France-KLM-Lounge-Terminal-5-Concourse-M-aVbBz76vzT       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/PEK       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/PHX       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/SHA       0.00      0.00      0.00         0\n",
      "                                                            /lounge-access/the-platinum-card/STL       0.00      0.00      0.00         0\n",
      "                      /lounge-access/the-platinum-card/STL/The-Pasta-House-Terminal-1-K1cu5Ecukw       0.00      0.00      0.00         0\n",
      "                                                       /myca/intl/paybill/emea/payBillPayment.do       0.00      0.00      0.00         0\n",
      "                                                                                         /offers       0.00      0.00      0.00         0\n",
      "                                                                /offers/3egbPPIRitvrkC2WXZLS4w==       0.00      0.00      0.00         0\n",
      "                                                                                /offers/eligible       0.00      0.00      0.00         0\n",
      "                                                                          /offers/eligible/error       0.00      0.00      0.00         1\n",
      "                                                                                /offers/enrolled       0.00      0.00      0.00         0\n",
      "                                                                                   /offers/error       0.00      0.00      0.00         0\n",
      "                                                                                   /offers/group       0.00      0.00      0.00         0\n",
      "                                                                                  /offers/group/       0.00      0.00      0.00         0\n",
      "                                                          /offers/group/i_REG8YZt1GFigpvX4jhpg==       0.00      0.00      0.00         0\n",
      "                                                                                /offers/redeemed       0.00      0.00      0.00         0\n",
      "                                                                                      /paperless       0.00      0.00      0.00         0\n",
      "                                                                               /payments/history       0.00      0.00      0.00         0\n",
      "                                                                              /payments/history/       0.00      0.00      0.00         0\n",
      "                                                                                     /promotions       0.00      0.00      0.00         0\n",
      "                                                                                        /rewards       0.00      0.00      0.00         0\n",
      "                                                                               /rewards/campaign       0.00      0.00      0.00         3\n",
      "                                                                             /rewards/gift-cards       0.00      0.00      0.00         0\n",
      "                                                                                  /rewards/learn       0.00      0.00      0.00         1\n",
      "                                                                                 /rewards/offers       0.00      0.00      0.00         0\n",
      "                                                                        /rewards/pay-with-points       0.00      0.00      0.00         0\n",
      "                                                                  /rewards/pay-with-points/error       0.00      0.00      0.00         0\n",
      "                                                                                 /rewards/wallet       0.00      0.00      0.00         0\n",
      "                                                                           /rewards/wallet/error       0.00      0.00      0.00         0\n",
      "                                                                                         /search       0.00      0.00      0.00         0\n",
      "                                                                                     /statements       0.00      0.00      0.00         0\n",
      "                                                                 /transaction/320180160596322135       0.00      0.00      0.00         0\n",
      "                                                                 /transaction/320180190636375689       0.00      0.00      0.00         0\n",
      "                                                                 /transaction/320180400959417110       0.00      0.00      0.00         0\n",
      "                                                                 /transaction/320180780552571400       0.00      0.00      0.00         0\n",
      "                                                                 /transaction/320180800584749175       0.00      0.00      0.00         0\n",
      "                                                                 /transaction/320180810601762802       0.00      0.00      0.00         0\n",
      "                                                                 /transaction/320180820611752538       0.00      0.00      0.00         0\n",
      "                                                                 /transaction/320180820616915307       0.00      0.00      0.00         0\n",
      "                                                                 /transaction/320181100055585997       0.00      0.00      0.00         0\n",
      "                                                                 /transaction/320181120081791417       0.00      0.00      0.00         0\n",
      "                                                                 /transaction/320181130101805957       0.00      0.00      0.00         0\n",
      "                                                                 /transaction/320181150133419482       0.00      0.00      0.00         0\n",
      "                                                                 /transaction/320181200219779545       0.00      0.00      0.00         0\n",
      "                                                                 /transaction/320181240274821022       0.00      0.00      0.00         0\n",
      "                                                                 /transaction/320181260313994038       0.00      0.00      0.00         0\n",
      "                                                                 /transaction/320181260320567182       0.00      0.00      0.00         0\n",
      "                                                                 /transaction/320181270325694908       0.00      0.00      0.00         0\n",
      "                                                           /transaction/320181270325694908/error       0.00      0.00      0.00         0\n",
      "                                                                 /transaction/320181270327117532       0.00      0.00      0.00         0\n",
      "                                                                 /transaction/320181270329623427       0.00      0.00      0.00         0\n",
      "                                                            /transaction/P0014485320180507084318       0.00      0.00      0.00         0\n",
      "                                                            /transaction/P0020285320180507164801       0.00      0.00      0.00         0\n",
      "                                                            /transaction/P0024345120180508095602       0.00      0.00      0.00         0\n",
      "                                                            /transaction/P0024453020180507082559       0.00      0.00      0.00         0\n",
      "                                                      /transaction/P0024453020180507082559/error       0.00      0.00      0.00         0\n",
      "                                                            /transaction/P0050440420180506145540       0.00      0.00      0.00         0\n",
      "                                                            /transaction/P0056169420180508092521       0.00      0.00      0.00         0\n",
      "                                                            /transaction/P0118517720180507120259       0.00      0.00      0.00         0\n",
      "                                                                                   /transactions       0.00      0.00      0.00         0\n",
      "                                                                             /transactions/error       0.12      0.31      0.18        13\n",
      "                                                              /us/small-business/shop-small/maps       0.00      0.00      0.00         0\n",
      "                                              /us/small-business/shop-small/maps/cat=Newly-Added       0.00      0.00      0.00         0\n",
      "                                          /us/small-business/shop-small/maps/guides/Chicago-Eats       0.00      0.00      0.00         0\n",
      "                                        /us/small-business/shop-small/maps/guides/San-Diego-Eats       0.00      0.00      0.00         0\n",
      "                     /us/small-business/shop-small/maps/guides/The-Shop-Small-Guide-to-Nashville       0.00      0.00      0.00         0\n",
      "\n",
      "                                                                                     avg / total       0.88      0.78      0.83      1824\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(res, test_y, target_names=list(le.classes_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn import hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function distribute_covar_matrix_to_match_covariance_type is deprecated; The function distribute_covar_matrix_to_match_covariance_typeis deprecated in 0.18 and will be removed in 0.20.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "e:\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function log_multivariate_normal_density is deprecated; The function log_multivariate_normal_density is deprecated in 0.18 and will be removed in 0.20.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "e:\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function log_multivariate_normal_density is deprecated; The function log_multivariate_normal_density is deprecated in 0.18 and will be removed in 0.20.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "e:\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function log_multivariate_normal_density is deprecated; The function log_multivariate_normal_density is deprecated in 0.18 and will be removed in 0.20.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[1;34m(self, code_obj, result)\u001b[0m\n\u001b[0;32m   2962\u001b[0m                 \u001b[1;31m#rprint('Running code', repr(code_obj)) # dbg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2963\u001b[1;33m                 \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2964\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-322-76e953b62e22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhmm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGaussianHMM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\anaconda3\\lib\\site-packages\\hmmlearn\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, lengths)\u001b[0m\n\u001b[0;32m    435\u001b[0m                     \u001b[0mstats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframelogprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposteriors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfwdlattice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m                     bwdlattice)\n\u001b[0m\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\lib\\site-packages\\hmmlearn\\hmm.py\u001b[0m in \u001b[0;36m_accumulate_sufficient_statistics\u001b[1;34m(self, stats, obs, framelogprob, posteriors, fwdlattice, bwdlattice)\u001b[0m\n\u001b[0;32m    222\u001b[0m         super(GaussianHMM, self)._accumulate_sufficient_statistics(\n\u001b[1;32m--> 223\u001b[1;33m             stats, obs, framelogprob, posteriors, fwdlattice, bwdlattice)\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\lib\\site-packages\\hmmlearn\\base.py\u001b[0m in \u001b[0;36m_accumulate_sufficient_statistics\u001b[1;34m(self, stats, X, framelogprob, posteriors, fwdlattice, bwdlattice)\u001b[0m\n\u001b[0;32m    625\u001b[0m                                  bwdlattice, framelogprob, lneta)\n\u001b[1;32m--> 626\u001b[1;33m             \u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'trans'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogsumexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlneta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\lib\\site-packages\\numpy\\lib\\utils.py\u001b[0m in \u001b[0;36mnewfunc\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdepdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\lib\\site-packages\\scipy\\special\\_logsumexp.py\u001b[0m in \u001b[0;36mlogsumexp\u001b[1;34m(a, axis, b, keepdims, return_sign)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0ma_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = hmm.GaussianHMM(len(le.classes_))\n",
    "model.fit(train_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score as acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function log_multivariate_normal_density is deprecated; The function log_multivariate_normal_density is deprecated in 0.18 and will be removed in 0.20.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "e:\\anaconda3\\lib\\site-packages\\hmmlearn\\base.py:451: RuntimeWarning: divide by zero encountered in log\n",
      "  n_samples, n_components, np.log(self.startprob_),\n"
     ]
    }
   ],
   "source": [
    "pr = model.predict(train_data)\n",
    "acc(train_y, pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = model.predict(test_data)\n",
    "acc(test_y, pr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
